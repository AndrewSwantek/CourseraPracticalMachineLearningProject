temp <-summary(fit)
se <-temp$sigma*sqrt(temp$cov.unscaled[2,2]+temp$cov.unscaled[3,3]-2*temp$cov.unscaled[2,3])
t<-bbmbc / se
p <- pt(-abs(t),df=fit$df)
out <- c(bbmbc,se,t,p)
names(out) <- c("B-C", "SE", "T", "P")
round(out,3)
fit <- lm(count~spray,data=InsectSprays) #A is ref
bbmbc <-coef(fit)[2]-coef(fit)[3] # B-C
temp <-summary(fit)
se <-temp$sigma*sqrt(temp$cov.unscaled[2,2]+temp$cov.unscaled[3,3]-2*temp$cov.unscaled[2,3])
t<-bbmbc / se
p <- pt(-abs(t),df=fit$df)
out <- c(bbmbc,se,t,p)
names(out) <- c("B-C", "SE", "T", "P")
round(out,3)
y <- x1 + rnorm(n, sd = .3)
a <- summary(lm(y ~ x1))$cov.unscaled[2,2]
c(summary(lm(y ~ x1 + x2))$cov.unscaled[2,2],
summary(lm(y~ x1 + x2 + x3))$cov.unscaled[2,2]) / a
temp <- apply(betas, 1, var); temp[2 : 3] / temp[1]
n <- 100; nosim <- 1000
x1 <- rnorm(n); x2 <- x1/sqrt(2) + rnorm(n) /sqrt(2)
x3 <- x1 * 0.95 + rnorm(n) * sqrt(1 - 0.95^2);
betas <- sapply(1 : nosim, function(i){ #get x 1 coefficient from 1000 simualtions
y <- x1 + rnorm(n, sd = .3)
c(coef(lm(y ~ x1))[2],  #3 regression models
coef(lm(y ~ x1 + x2))[2],
coef(lm(y ~ x1 + x2 + x3))[2])
})
round(apply(betas, 1, sd), 5)
y <- x1 + rnorm(n, sd = .3)
a <- summary(lm(y ~ x1))$cov.unscaled[2,2]
c(summary(lm(y ~ x1 + x2))$cov.unscaled[2,2],
summary(lm(y~ x1 + x2 + x3))$cov.unscaled[2,2]) / a
temp <- apply(betas, 1, var); temp[2 : 3] / temp[1]
data(swiss);
fit1 <- lm(Fertility ~ Agriculture, data = swiss)
a <- summary(fit1)$cov.unscaled[2,2]
fit2 <- update(fit, Fertility ~ Agriculture + Examination)
fit3 <- update(fit, Fertility ~ Agriculture + Examination + Education)
c(summary(fit2)$cov.unscaled[2,2],
summary(fit3)$cov.unscaled[2,2]) / a
library(car)
data(swiss);
fit1 <- lm(Fertility ~ Agriculture, data = swiss)
a <- summary(fit1)$cov.unscaled[2,2]
fit2 <- update(fit1, Fertility ~ Agriculture + Examination)
fit3 <- update(fit1, Fertility ~ Agriculture + Examination + Education)
c(summary(fit2)$cov.unscaled[2,2],
summary(fit3)$cov.unscaled[2,2]) / a
library(car)
fit <- lm(Fertility ~ . , data = swiss)
vif(fit)
sqrt(vif(fit)) #I prefer sd
fit1 <- lm(Fertility ~ Agriculture, data = swiss)
fit3 <- update(fit, Fertility ~ Agriculture + Examination + Education)
fit5 <- update(fit, Fertility ~ Agriculture + Examination + Education + Catholic + Infant.Mortality)
anova(fit1, fit3, fit5)
swirl()
library(swirl)
swirl()
library(swirl)
swirl()
lm(child~parent,data=galton)
fit <- lm(child~parent,data=galton)
fit$residuals
summary(fit)
mean(fit$residuals)
cov(fit$residuals,galton$parent)
ols.ic<-fit$coef[1]
ols.slope<-fit$coef[2]
rhs-lhs
lhs-rhs
all.equal(lhs,rhs)
varChild<-var(galton$child)
varRes <- var(fit$residuals)
varEst <- est(fit)
varEst <- est(y)
varEst <- 2
varEst <-  var(est(ols.slope, ols.ic))
all.equal(varChild,sum(varRes,varEst))
all.equal(varChild,varRes+varEst)
efit <- lm(accel~mag+dist,attenu)
mean(efit$resisudals)
mean(efit$residuals)
cov(efit$residuals,attenu$mag)
cov(efit$residuals,attenu$dist)
exit9)
exit
exit()
close()
data(mtcars)
names(mtcars)
lm <-lm(mpg~cyl+wt,data=mtcars)
summary(lm)
-1.5078*2
-1.5078*4
lm <-lm(mpg~cyl,data=mtcars)
summary(lm)
lm <-lm(mpg~cyl,data=mtcars)
lm <-lm(mpg~cyl+wt,data=mtcars)
summary(lm)
lm <-lm(mpg~cyl,data=mtcars)
summary(lm)
lm <-lm(mpg~factor(cyl)+wt,data=mtcars)
summary(lm)
lm <-lm(mpg~factor(cyl),data=mtcars)
summary(lm)
lm1<-lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars))
lm1<-lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
summary(lm1)
?mtcars
mtcars$wt[1]
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit1<-lm(y~x)
hatvalues(fit1)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit1<-lm(y~x)
dfbetas(fit1)
lm1 <-lm(mpg~factor(cyl)+wt,data=mtcars)
lm2 <-lm(mpg~factor(cyl):wt,data=mtcars)
anova(lm1,lm2)
lm2 <-lm(mpg~factor(cyl)*wt,data=mtcars)
anova(lm1,lm2)
?anova
?log
log(1/2,base=2)
log(1/2,base=2)*1/2*2
data(iris); library(ggplot2)
names(iris)
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
data(iris); library(ggplot2);library(caret)
names(iris)
table(iris$Species)
#We want to predict species
#Create training and test set
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
qplot(Petal.Width,Sepal.Width,colour=Species,data=training)
modFit <- train(Species ~ .,method="rpart",data=training)
#outcome is species using all variables
#method is rpart (regression and classification trees)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
install.packages("rattle")
library(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages(rattle,dependencies=c("Depends","Suggests"))
install.packages("rattle",dependencies=c("Depends","Suggests"))
install.packages(rattle,dependencies=c("Depends","Suggests"))
install.packages("rattle",dependencies=c("Depends","Suggests"))
data(iris); library(ggplot2);library(caret)
names(iris)
table(iris$Species)
#We want to predict species
#Create training and test set
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
#Plot petal widths/sepalwidths colored by species
qplot(Petal.Width,Sepal.Width,colour=Species,data=training)
#3 distinct clusters
#Train model
modFit <- train(Species ~ .,method="rpart",data=training)
#outcome is species using all variables
#method is rpart (regression and classification trees)
print(modFit$finalModel)
# n= 105
#
# node), split, n, loss, yval, (yprob)
# * denotes terminal node
#
# 1) root 105 70 setosa (0.33333333 0.33333333 0.33333333)
# 2) Petal.Length< 2.45 35  0 setosa (1.00000000 0.00000000 0.00000000) *      #Split based on if petal length is less than 2.45, all are species setosa
# 3) Petal.Length>=2.45 70 35 versicolor (0.00000000 0.50000000 0.50000000)
# 6) Petal.Length< 4.75 34  1 versicolor (0.00000000 0.97058824 0.02941176) *
# 7) Petal.Length>=4.75 36  2 virginica (0.00000000 0.05555556 0.94444444) *
#Plot of classification tree
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
#prettier version with rattle packages
library(rattle)
fancyRpartPlot(modFit$finalModel)
predict(modFit,newdata=testing)
library(ElemStatLearn)
library(ElemStatLearn); data(ozone,package="ElemStatLearn")
ozone <- ozone[order(ozone$ozone),]
head(ozone)
ll <- matrix(NA,nrow=10,ncol=155)
#loop over 10 different samples of data set, sampling with replacement
for(i in 1:10){
ss <- sample(1:dim(ozone)[1],replace=T)
#subset of data set from random sample
ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),] #reorder by ozone variable everytime
#fit a smooth curve relating temp to ozone
loess0 <- loess(temperature ~ ozone,data=ozone0,span=0.2) #span is a measure of how smooth the curve will be
#predict for every loess0 cuve on new data set
ll[i,] <- predict(loess0,newdata=data.frame(ozone=1:155)) #we will then average these values
}
#plot this
plot(ozone$ozone,ozone$temperature,pch=19,cex=0.5)
for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
lines(1:155,apply(ll,2,mean),col="red",lwd=2)
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
treebag <- bag(predictors, temperature, B = 10,
bagControl = bagControl(fit = ctreeBag$fit,
predict = ctreeBag$pred,
aggregate = ctreeBag$aggregate))
plot(ozone$ozone,temperature,col='lightgrey',pch=19)
points(ozone$ozone,predict(treebag$fits[[1]]$fit,predictors),pch=19,col="red")
points(ozone$ozone,predict(treebag,predictors),pch=19,col="blue")
ctreeBag$fit
data(iris); library(ggplot2)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
library(caret)
modFit <- train(Species~ .,data=training,method="rf",prox=TRUE) #mehod='rf' is for random forests
modFit
getTree(modFit$finalModel,k=2)
irisP <- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox)
irisP <- as.data.frame(irisP); irisP$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col=Species,data=training)
p + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
pred <- predict(modFit,testing); testing$predRight <- pred==testing$Species
table(pred,testing$Species)
qplot(Petal.Width,Petal.Length,colour=predRight,data=testing,main="newdata Predictions")
library(ISLR); data(Wage); library(ggplot2); library(caret);
Wage <- subset(Wage,select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage,
p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
modFit <- train(wage ~ ., method="gbm",data=training,verbose=FALSE) #verbose=False to not print a lot of output.
print(modFit)
qplot(predict(modFit,testing),wage,data=testing)
data(iris); library(ggplot2)
names(iris)
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
#build the lda model
modlda = train(Species ~ .,data=training,method="lda")
#build a naive bayes model
modnb = train(Species ~ ., data=training,method="nb")
#build the lda model
modlda = train(Species ~ .,data=training,method="lda")
#build a naive bayes model
modnb = train(Species ~ ., data=training,method="nb")
plda = predict(modlda,testing); pnb = predict(modnb,testing)
table(plda,pnb)
equalPredictions = (plda==pnb)
qplot(Petal.Width,Sepal.Width,colour=equalPredictions,data=testing)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
training <- segmentationOriginal[segmentationOriginal$Case,]; testing <- Wage[-segmentationOriginal$Case,]
training <- segmentationOriginal[segmentationOriginal$Case,]; testing <- segmentationOriginal[-segmentationOriginal$Case,]
names(segmentation)
names(segmentationOriginal)
unique(segmentationOriginal$Case)
training <- segmentationOriginal[segmentationOriginal$Case==Train,]; testing <- segmentationOriginal[segmentationOriginal$Case==Test,]
training <- segmentationOriginal[segmentationOriginal$Case=="Train",]; testing <- segmentationOriginal[segmentationOriginal$Case=="Test",]
?set.seed
set.seed(125)
modFit <- train(Class ~ .,method="rpart",data=training)
predict(modFit,newdata=data.Frame(TotalIntench2 = 23,000 FiberWidthCh1 = 10 PerimStatusCh1=2 ))
data.Frame(TotalIntench2 = 23,000 FiberWidthCh1 = 10, PerimStatusCh1=2 )
data.Frame(TotalIntench2 = 23000 FiberWidthCh1 = 10, PerimStatusCh1=2 )
data.Frame(TotalIntench2 = 23000, FiberWidthCh1 = 10, PerimStatusCh1=2 )
data.frame(TotalIntench2 = 23000, FiberWidthCh1 = 10, PerimStatusCh1=2 )
DD<-data.frame(TotalIntench2 = 23000, FiberWidthCh1 = 10, PerimStatusCh1=2 )
predict(modFit,newdata=DD)
library(rattle)
fancyRpartPlot(modFit$finalModel)
library(pgmm)
data(olive)
olive = olive[,-1]
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
modFit <- train(Area ~ .,method="rpart",data=newdata)
names(olive)
modFit <- train(Area ~ .,method="rpart",data=olive)
predict(modFit,newdata=newdata)
head(olive)
head(olive,20)
tail(olive,20)
?olive
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
names(trainSA)
modFit<-train(chd ~ .,method="glm", family="binomial",data=trainSA)
modFit<-train(chd ~ age+alcohol+obesity+tobacco+typea+ldl,method="glm", family="binomial",data=trainSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(trainSA$chd, modFit)
missClass(trainSA$chd, predict(modFit,trainSA))
missClass(trainSA$chd, predict(modFit,testSA))
missClass(testSA$chd, predict(modFit,testSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
head(vowel.train)
head(vowel.test)
dim(vowel.train) # 528  11
dim(vowel.test) # 462  11
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
order(varImp(modelRf), decreasing=T)
modelRf <- predict(y ~ ., data = vowel.train,method="rf",prox=TRUE)
modelRf <- train(y ~ ., data = vowel.train,method="rf",prox=TRUE)
order(varImp(modelRf), decreasing=T)
varImp(modelRf)
g
library(ggplot2);library(UsingR);data(galton);library(reshape); long<-melt(galton)
g<-ggplot(long,aes(x=value),fill=variable)
g<-g+geom_histogram(colour="black",binwidth=1)
g<-g+facet_grid(.~variable)
g
data(mtcars)
library(ggplot2)
ggpairs(mtcars)
plotmatrix(mtcars)
library(GGally)
ggpairs(mtcars)
pairs(mtcars)
pairs(mtcars, panel=panel.smooth, main="Pair Graph of Motor Trend Car Road Tests")
?mtcars
data(mtcars)
summary(mtcars)
names(mtcars)
mtcars
mtcars$am
```{r loaddata, fig.height=5,cache=TRUE}
setwd("E:/Coursera/DataScienceTrack/8_PracticalMachineLearning/CourseProject")
trainRaw <- read.csv("./data/pml-training.csv")
testRaw <- read.csv("./data/pml-testing.csv")
trainRaw <- read.csv("pml-training.csv")
testRaw <- read.csv("pml-testing.csv")
names(TrainRaw)
names(trainRaw)
trainRaw2 <- trainRaw[,8:]
trainRaw2 <- trainRaw[,8:dim(trainRaw)[2]]
names(trainRaw2)
DataIn <- DataIn[,8:dim(DataIn)[2]]
DataIn <- read.csv("pml-training.csv")
DataIn <- DataIn[,8:dim(DataIn)[2]]
colsums(DataIn)
colSums(DataIn)
colSums(is.na(DataIn))
is.na(DataIn)
DataIn <- DataIn[, colSums(is.na(DataIn)) == 0]
colsums(DataIn)
colSums(is.na(DataIn))
summary(DataIn)
trainRemove <- grepl("^X|timestamp|window", names(DataIn))
trainRemove
Data <- DataIn[,8:dim(DataIn)[2]]
#remove columns that have NAs
Data <- Data[, colSums(is.na(Data)) == 0]
DataIn <- read.csv("pml-training.csv", na.strings = c("NA", ""))
Data <- DataIn[,8:dim(DataIn)[2]]
Data <- Data[, colSums(is.na(Data)) == 0]
head(DataIn)
?corrplot
library(corrplot)
?corrplot
Plot_corr <- cor(Data[, -dim(DataIn)[2]])
corrplot(Plot_corr, method="shade")
Plot_corr <- cor(Data[, -dim(Data)[2]])
corrplot(Plot_corr, method="shade")
corrplot(Plot_corr, method="square")
corrplot(Plot_corr, method="ellipse")
corrplot(Plot_corr, method="pie")
corrplot(Plot_corr, method="circle")
corrplot(Plot_corr)
Plot_corr <- cor(Data)
inTrain <- createDataPartition(y=Data$classe, p=0.66, list=FALSE)
library(caret)
inTrain <- createDataPartition(y=Data$classe, p=0.66, list=FALSE)
DataTraining <- training[inTrain, ]; DataTesting <- training[-inTrain, ]
inTrain <- createDataPartition(y=Data$classe, p=0.66, list=FALSE)
DataTraining <- Data[inTrain, ]; DataTesting <- Data[-inTrain, ]
nrows(DataTraining)
nrows(DataTesting)
nrow(DataTraining)
nrow(DataTesting)
inTrain <- createDataPartition(y=Data$classe, p=0.66, list=FALSE)
DataTraining <- Data[inTrain, ]; DataTesting <- Data[-inTrain, ]
nrow(DataTraining)
nrow(DataTesting)
modFit <- train(classe~ .,data=DataTraining,method="rf",prox=TRUE) #mehod='rf' is for random forests
?randomForest
modFit <- train(classe~ .,data=DataTraining,method="rf",prox=TRUE, trControl=trainControl(method = "cv", number = 6),ntree=200)
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
modFit <- randomForest(classe~ .,data=DataTraining,method="rf",prox=TRUE, trControl=trainControl(method = "cv", number = 6),ntree=200)
modFit <- randomForest(classe~ .,data=DataTraining,method="rf",prox=TRUE, trControl=trainControl(method = "cv", number = 6),ntree=300)
TrainingPredict <- predict(modFit, DataTraining, type = "class")
confusionMatrix(TrainingPredict)
confusionMatrix(DataTraining$classe,TrainingPredict)
?predict
TestingPredict <- predict(modFit, DataTesting, type = "class")
confusionMatrix(DataTesting$classe,TestingPredict)
modFit <- randomForest(classe~ .,data=DataTraining,method="rf",prox=TRUE, trControl=trainControl(method = "cv", number = 6),ntree=500)
TrainingPredict <- predict(modFit, DataTraining, type = "class")
confusionMatrix(DataTesting$classe,TestingPredict)
confusionMatrix(DataTraining$classe,TrainingPredict)
modFit <- randomForest(classe~ .,data=DataTraining,method="rf",prox=TRUE, trControl=trainControl(method = "cv", number = 6)),ntree=150)
modFit <- randomForest(classe~ .,data=DataTraining,method="rf",prox=TRUE, trControl=trainControl(method = "cv", number = 6)),ntree=150))
modFit <- randomForest(classe~ .,data=DataTraining,method="rf",prox=TRUE, trControl=trainControl(method = "cv", number = 6),ntree=150)
TrainingPredict <- predict(modFit, DataTraining, type = "class")
confusionMatrix(DataTraining$classe,TrainingPredict)
TestingPredict <- predict(modFit, DataTesting, type = "class")
confusionMatrix(DataTesting$classe,TestingPredict)
modFit <- randomForest(classe~ .,data=DataTraining,method="rf",prox=TRUE, trControl=trainControl(method = "cv", number = 6)),ntree=100)
modFit <- randomForest(classe~ .,data=DataTraining,method="rf",prox=TRUE, trControl=trainControl(method = "cv", number = 6),ntree=100)
TestingPredict <- predict(modFit, DataTesting, type = "class")
confusionMatrix(DataTesting$classe,TestingPredict)
names(DataTraining)
tt=testRaw[,names(DataTraining)]
tt=testRaw[,c(names(DataTraining))]
tt=testRaw[,[names(DataTraining)]]
tt=testRaw[[names(DataTraining)]]
tt=testRaw[names(DataTraining)]
testRaw[[names(DataTraining)[1]]]
testRaw[[names(DataTraining)]]
testRaw[[,names(DataTraining)]]
df <- data.frame( ABC_1 = runif(3),
ABC_2 = runif(3),
XYZ_1 = runif(3),
XYZ_2 = runif(3) )
grepl( "ABC" , names( df ) )
grepl(names(DataTesting), names( testRaw ) )
sum(grepl(names(DataTesting), names( testRaw ) ))
names(DataTesting)%in% names( testRaw )
names(testRaw)%in% names( DataTesting )
sum(names(testRaw)%in% names( DataTesting ))
vect<-names(testRaw)%in% names( DataTesting )
vect
head(testRaw[,vect])
DataTest <- read.csv("pml-testing.csv", na.strings = c("NA", ""))
#index based on the names
DataFinalTest <- DataTest[,vect]
result <- predict(modFit, DataFinalTest)
result
prp(modFit)
fancyRpartPlot(modFit$finalModel)
library(rattle)
library(rpart.plot)
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit)
fancyRpartPlot(modFit)
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(result)
corrplot
?corrplot
corrplot(Plot_corr,col=jetColors(200))
jetColors(100)
corrplot(Plot_corr,col=rainbow)
corrplot(Plot_corr,col=rainbow(200))
corrplot(Plot_corr,col=jet(200))
corrplot(Plot_corr,col=heat.colors(200))
corrplot(Plot_corr,col=terrain.colors(200))
corrplot(Plot_corr,col=topo.colors(200))
corrplot(Plot_corr,col=heat.colors(200))
corrplot(Plot_corr,col=rainbow(200))
corrplot(Plot_corr,col=gray(200))
corrplot(Plot_corr,col=gray.colors(200))
corrplot(Plot_corr,col=gray.colors(200),method='color')
corrplot(Plot_corr,col=gray.colors(200),method='square')
corrplot(Plot_corr,col=gray.colors(200),method='circle')
corrplot(Plot_corr,col=cm.colors(200),method='circle')
